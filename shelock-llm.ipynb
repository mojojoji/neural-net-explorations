{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581863"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset/holmes.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation and training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 135485, Validation data: 15054\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "data = torch.tensor(enc.encode(text)).long()\n",
    "\n",
    "training_data_length = int(len(data) * 0.9) # 90% of the data\n",
    "\n",
    "train_data = data[:training_data_length]\n",
    "validation_data = data[training_data_length:]\n",
    "\n",
    "print(f'Training data: {len(train_data)}, Validation data: {len(validation_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random generator for getting same output everytime\n",
    "torch.manual_seed(12341)\n",
    "\n",
    "# Number of input tokens to consider for prediction\n",
    "context_length = 8\n",
    "\n",
    "# Number of samples to consider in one batch. Each sample will be a sequence of context_length tokens\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    # Generate batch number times random indices in the data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    # Get input tensor for each random index with length context_length\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    # Get output tensor. Each element is the next token prediction for the corresponding input tensor\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = enc.n_vocab # Total number of tokens in the vocabulary for the tiktoken encoding\n",
    "embedding_dim = 16 # Dimension of the embedding vector per token. Each token will be converted to this size vector and later will be transformed to have inner meaning\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SherlockModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding layer to convert batch and example to embedding vectors\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer to convert embedding vectors to logits that represent the probability of each token in the vocabulary.\n",
    "        self.linear_layer = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets = None):\n",
    "        # Forward pass\n",
    "        token_embeddings = self.embedding_layer(x) # (B, T, C)\n",
    "        logits = self.linear_layer(token_embeddings) # (B, T, C) C = vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape logits and targets to calculate loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (B*T, C)\n",
    "            targets = targets.view(B*T) # (B*T)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens = 10):\n",
    "        # Generate a sequence of tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass\n",
    "            logits, loss = self(x)\n",
    "            # get last token logits from each batch (Removing the time dimension or the token traversal dimension)\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "\n",
    "            # convert logits to probabilities (by applying softmax and squishing the values between 0 and 1)\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "\n",
    "            # from the next token probabilities, sample a token\n",
    "            next_token_indices = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "\n",
    "            # Append the sampled token to the input tensor for the next iteration\n",
    "            x = torch.cat((x, next_token_indices), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.298051834106445\n"
     ]
    }
   ],
   "source": [
    "model = SherlockModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for steps in range(1000):\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! River Fishingprototype residency Military DinosaurprEditorBenef Contact\n"
     ]
    }
   ],
   "source": [
    "input = torch.zeros((1,1), dtype=torch.long)\n",
    "output = enc.decode(model.generate(input)[0].tolist())\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
